<!doctype html>
<html lang="zh">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>你的论文标题</title>
  <meta name="description" content="单页项目展示">
  <link rel="stylesheet" href="/assets/css/styles.css">
</head>
<body>
  <header class="site-header">
    <div class="container header-inner">
      <div class="brand">你的论文标题</div>
      <!-- 右上角四个胶囊按钮 -->

<!--       <nav class="nav actions">
        <a class="pill demo" href="https://your-demo.example.com" target="_blank" rel="noopener">Demo</a>
        <a class="pill" href="/assets/paper.pdf" target="_blank" rel="noopener">PDF</a>
        <a class="pill" href="https://github.com/your-username/gemholm" target="_blank" rel="noopener">Code</a>
        <a class="pill" href="https://arxiv.org/abs/xxxx.xxxxx" target="_blank" rel="noopener">arXiv</a>
      </nav> -->


<nav class="nav actions">
  <a class="pill demo" href="https://your-demo.example.com" target="_blank" rel="noopener">Demo</a>
  <a class="pill"      href="/assets/paper.pdf" target="_blank" rel="noopener">PDF</a>
  <a class="pill"      href="https://github.com/ddmeer/gemholm" target="_blank" rel="noopener">Code</a>
  <a class="pill"      href="https://arxiv.org/abs/xxxx.xxxxx" target="_blank" rel="noopener">arXiv</a>
</nav>




    </div>
  </header>

  <main class="site-main">
    <!-- 1) Hero：轮播（读取 _data/hero.yml） -->
<section class="section hero">
  <div class="container">
    <h1>你的论文标题</h1>
    <p class="authors">作者A<sup>1</sup>，作者B<sup>2</sup>，作者C<sup>1</sup></p>
    <p class="affiliations"><sup>1</sup>单位一　<sup>2</sup>单位二</p>

    
    <div
      class="hero-slider card"
      data-autoplay="true"
      data-interval="3500"
      aria-label="Hero image slider"
    >
      <div class="slides" aria-live="polite">
        
        <div class="slide is-active">
          <img src="/assets/images/hero/hero1.jpg" alt="主图 11">
          
          <div class="caption"></div>
          
        </div>
        
        <div class="slide">
          <img src="/assets/images/hero/hero2.jpg" alt="主图 22">
          
          <div class="caption"></div>
          
        </div>
        
        <div class="slide">
          <img src="/assets/images/hero/hero3.jpg" alt="主图 33">
          
          <div class="caption"></div>
          
        </div>
        
      </div>
      <button class="nav prev" aria-label="Previous">‹</button>
      <button class="nav next" aria-label="Next">›</button>
      <div class="dots"></div>
    </div>
  </div>
</section>

<!-- 2) Abstract：MD → HTML -->
<section id="abstract" class="section gray">
  <div class="container">
    <h2 class="h2-center">Abstract</h2>
    <div class="lead">
      
      <p>我们提出……（在这里写摘要，建议 1–2 段）<br />
可用<strong>粗体</strong>、<em>斜体</em>、以及行内代码 <code class="language-plaintext highlighter-rouge">x = y</code>。<br />
公式建议直接贴图（为保证跨端渲染一致性）。</p>

<p>The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
our model establishes a new single-model state-of-the-art BLEU score of 41.8 after
training for 3.5 days on eight GPUs, a small fraction of the training costs of the
best models from the literature. We show that the Transformer generalizes well to
other tasks by applying it successfully to English constituency parsing both with
large and limited training data.</p>

    </div>
  </div>
</section>

<!-- 3) Background：MD → HTML -->
<section id="background" class="section">
  <div class="container">
    <h2>Background</h2>
    <p class="subtle">简述研究背景、相关工作或挑战点。</p>
    
    <h3 id="研究背景">研究背景</h3>
<p>这里写背景/相关工作/挑战点的文字说明……</p>

<p>The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU
[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building
block, computing hidden representations in parallel for all input and output positions. In these models,
the number of operations required to relate signals from two arbitrary input or output positions grows
in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes
it more difficult to learn dependencies between distant positions [12]. In the Transformer this is
reduced to a constant number of operations, albeit at the cost of reduced effective resolution due
to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as
described in section 3.2.
Self-attention, sometimes called intra-attention is an attention mechanism relating different positions
of a single sequence in order to compute a representation of the sequence. Self-attention has been
used successfully in a variety of tasks including reading comprehension, abstractive summarization,
textual entailment and learning task-independent sentence representations [4, 27, 28, 22].
End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-
aligned recurrence and have been shown to perform well on simple-language question answering and
language modeling tasks [34].
To the best of our knowledge, however, the Transformer is the first transduction model relying
entirely on self-attention to compute representations of its input and output without using sequence-
aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate
self-attention and discuss its advantages over models such as [17, 18] and [9].</p>

<div class="card pad">
  <!-- 三列网格；想更宽就把 cols-3 改成 cols-4，或把 --min 调大/调小 -->
  <div class="fig-grid cols-3">
    <figure class="fig fig--input">
      <img src="/assets/images/background/bg-01.jpg" alt="输入/示意图 1" />
      <figcaption>输入/示意图 1</figcaption>
    </figure>

    <figure class="fig">
      <img src="/assets/images/background/bg-02.jpg" alt="相关工作 A" />
      <figcaption>相关工作 A</figcaption>
    </figure>

    <figure class="fig">
      <img src="/assets/images/background/bg-03.jpg" alt="相关工作 B" />
      <figcaption>相关工作 B</figcaption>
    </figure>

    <!-- 继续复制 figure 块即可添加更多图 -->
  </div>
</div>

<p>The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU
[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building
block, computing hidden representations in parallel for all input and output positions. In these models,
the number of operations required to relate signals from two arbitrary input or output positions grows
in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes
it more difficult to learn dependencies between distant positions [12]. In the Transformer this is
reduced to a constant number of operations, albeit at the cost of reduced effective resolution due
to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as
described in section 3.2.
Self-attention, sometimes called intra-attention is an attention mechanism relating different positions
of a single sequence in order to compute a representation of the sequence. Self-attention has been
used successfully in a variety of tasks including reading comprehension, abstractive summarization,
textual entailment and learning task-independent sentence representations [4, 27, 28, 22].
End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-
aligned recurrence and have been shown to perform well on simple-language question answering and
language modeling tasks [34].
To the best of our knowledge, however, the Transformer is the first transduction model relying
entirely on self-attention to compute representations of its input and output without using sequence-
aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate
self-attention and discuss its advantages over models such as [17, 18] and [9].</p>

  </div>
</section>

<!-- 4) Approach：MD → HTML -->
<section id="approach" class="section">
  <div class="container">
    <h2>Approach</h2>
    <p class="subtle">概述方法框架、关键模块与创新点。</p>
    
    <h3 id="方法简介">方法简介</h3>
<p>整体流程如图，包含 A、B、C 三个模块……（这里写你的方法文字）</p>

<div class="card edge">
  <!-- square=正方形，full=铺满整行，s-180=更紧凑；想更密就换 s-160，想稍疏一点用 s-200 -->
  <div class="fig-grid square full hovercap s-180">




<figure class="fig">
  <a href="/assets/images/approach/app-01.jpg" data-lightbox="approach">
    <img src="/assets/images/approach/app-01.jpg" alt="" />
  </a>
  <figcaption>说明文字</figcaption>
</figure>

<figure class="fig">
  <a href="/assets/images/approach/app-02.jpg" data-lightbox="approach">
    <img src="/assets/images/approach/app-02.jpg" alt="" />
  </a>
  <figcaption>说明文字</figcaption>
</figure>


<figure class="fig">
  <a href="/assets/images/approach/app-03.jpg" data-lightbox="approach">
    <img src="/assets/images/approach/app-03.jpg" alt="" />
  </a>
  <figcaption>说明文字</figcaption>
</figure>



    <!-- 继续复制 figure 即可添加更多图 -->
    <!--
    <figure class="fig">
      <img src="/assets/images/approach/app-04.jpg" alt="">
      <figcaption>说明文字</figcaption>
    </figure>
    <figure class="fig">
      <img src="/assets/images/approach/app-05.jpg" alt="">
      <figcaption>说明文字</figcaption>
    </figure>
    <figure class="fig">
      <img src="/assets/images/approach/app-06.jpg" alt="">
      <figcaption>说明文字</figcaption>
    </figure>
    -->

  </div>
</div>

  </div>
</section>





<!-- 5) Results：读取 _data/results.yml 的图墙（带放大） -->
<!-- 5) Results -->


<!-- 5) Results：由 results.md 驱动 -->
<section id="results" class="section">
  <div class="container">
    <h2>Results</h2>
    <p class="subtle">展示定性/定量结果，支持 PDF 预览与说明文字。</p>

    
    <h3 id="poster--paper">Poster / Paper</h3>

<div class="card pad">
  <!-- 本地 PDF：assets/papers/paper.pdf -->
  <iframe class="pdf-embed" src="/assets/papers/paper.pdf#toolbar=1&amp;navpanes=0&amp;view=FitH" loading="lazy"></iframe>

  <!-- 兜底：如果浏览器禁用内联预览，显示下载/打开链接 -->
  <p style="margin-top:.75rem;text-align:center;">
    如果没有显示预览，<a href="/assets/papers/paper.pdf" target="_blank" rel="noopener">点击这里打开/下载 PDF</a>
  </p>
</div>

<!-- 你还可以在这里继续写其它结果说明、图表等 Markdown 内容 -->

  </div>
</section>











<!-- 6) Societal Impact：MD → HTML -->
<section id="impact" class="section tight gray">
  <div class="container">
    <h2>Societal Impact</h2>
    <p class="subtle">潜在社会影响、伦理考量、误用风险与缓解策略。</p>
    
    <h3 id="societal-impact">Societal Impact</h3>
<ul>
  <li>潜在积极影响：……</li>
  <li>风险与误用：……</li>
  <li>缓解措施与负责任发布策略：……</li>
</ul>

  </div>
</section>

  </main>

  <footer class="site-footer">
    <div class="container footer-inner">
      <span>© 2025 Your Name</span>
      <span class="sep">•</span>
      <a href="https://github.com/your-username/gemholm" target="_blank" rel="noopener">GitHub</a>
    </div>
  </footer>

  <!-- 轻量 Lightbox（保留，便于后续结果图放大） -->
  <div id="lightbox" class="lightbox" aria-hidden="true">
    <img alt="预览大图">
  </div>
  <script>
  (function () {
    const overlay = document.getElementById('lightbox');
    const overlayImg = overlay.querySelector('img');
    document.addEventListener('click', (e) => {
      const a = e.target.closest('a[data-lightbox]');
      if (a) {
        e.preventDefault();
        overlayImg.src = a.getAttribute('href');
        overlay.classList.add('show');
        overlay.setAttribute('aria-hidden', 'false');
      }
      if (e.target === overlay) {
        overlay.classList.remove('show');
        overlay.setAttribute('aria-hidden', 'true');
        overlayImg.src = '';
      }
    });
  })();
  </script>

  <!-- Hero 轮播脚本（零依赖） -->
  <script>
  (function () {
    document.querySelectorAll('.hero-slider').forEach(function (root) {
      var slides = Array.from(root.querySelectorAll('.slide'));
      if (slides.length <= 1) return;

      // 生成圆点
      var dotsWrap = root.querySelector('.dots') || document.createElement('div');
      dotsWrap.classList.add('dots'); dotsWrap.innerHTML = '';
      slides.forEach(function (_, i) {
        var b = document.createElement('button');
        b.className = 'dot' + (i === 0 ? ' is-active' : '');
        b.setAttribute('aria-label', 'Go to slide ' + (i + 1));
        b.addEventListener('click', function () { go(i); });
        dotsWrap.appendChild(b);
      });
      if (!root.querySelector('.dots')) root.appendChild(dotsWrap);
      var dots = Array.from(dotsWrap.querySelectorAll('.dot'));

      var prevBtn = root.querySelector('.prev');
      var nextBtn = root.querySelector('.next');
      var idx = 0;
      var autoplay = root.dataset.autoplay === 'true';
      var interval = parseInt(root.dataset.interval || '3500', 10);
      var timer = null;

      function setActive(i) {
        slides.forEach(function (s, k) { s.classList.toggle('is-active', k === i); });
        dots.forEach(function (d, k) { d.classList.toggle('is-active', k === i); });
      }
      function go(i) { idx = (i + slides.length) % slides.length; setActive(idx); restart(); }
      function next() { go(idx + 1); }
      function prev() { go(idx - 1); }

      prevBtn && prevBtn.addEventListener('click', prev);
      nextBtn && nextBtn.addEventListener('click', next);

      function start() { if (!autoplay) return; stop(); timer = setInterval(next, interval); }
      function stop()  { if (timer) { clearInterval(timer); timer = null; } }
      function restart(){ if (autoplay) { stop(); start(); } }

      root.addEventListener('mouseenter', stop);
      root.addEventListener('mouseleave', start);
      document.addEventListener('visibilitychange', function () { document.hidden ? stop() : start(); });

      // 触摸滑动
      var startX = null;
      root.addEventListener('touchstart', function (e) { startX = e.touches[0].clientX; }, {passive:true});
      root.addEventListener('touchend', function (e) {
        if (startX == null) return;
        var dx = e.changedTouches[0].clientX - startX; startX = null;
        var threshold = 35;
        if (dx > threshold) prev(); else if (dx < -threshold) next();
      });

      // 键盘左右键
      root.addEventListener('keydown', function (e) {
        if (e.key === 'ArrowLeft') prev();
        if (e.key === 'ArrowRight') next();
      });
      root.setAttribute('tabindex', '0');

      start();
    });
  })();
  </script>
</body>
</html>
